{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn.cluster\n",
    "import sklearn.neighbors\n",
    "\n",
    "from util import config\n",
    "from util import mapping\n",
    "from util import road_backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove weird rides - DBSCAN\n",
    "\n",
    "GPS-recorded rides can have some issues with gaps in the data.  This could be an issue with the machine, or an issue with the human, e.g. forgetting to turn the unit back on for a while after pausing.\n",
    "\n",
    "Here, look for long gaps in each ride, and take the longest continuous segment out of each.\n",
    "\n",
    "To find continuous segments, use DBSCAN\n",
    "\n",
    "* Want to extract contiguous segments, so density-based clustering is ideal\n",
    "* DBSCAN is the best density-based option, because it is intuitive to define 'epsilon' (maximum distance between points in this cluster) in geographic space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 19570\n",
      "200 of 19570\n",
      "400 of 19570\n",
      "600 of 19570\n",
      "800 of 19570\n",
      "1000 of 19570\n",
      "1200 of 19570\n",
      "1400 of 19570\n",
      "1600 of 19570\n",
      "1800 of 19570\n",
      "2000 of 19570\n",
      "2200 of 19570\n",
      "2400 of 19570\n",
      "2600 of 19570\n",
      "2800 of 19570\n",
      "3000 of 19570\n",
      "3200 of 19570\n",
      "3400 of 19570\n",
      "3600 of 19570\n",
      "3800 of 19570\n",
      "4000 of 19570\n",
      "4200 of 19570\n",
      "4400 of 19570\n",
      "4600 of 19570\n",
      "4800 of 19570\n",
      "5000 of 19570\n",
      "5200 of 19570\n",
      "5400 of 19570\n",
      "5600 of 19570\n",
      "5800 of 19570\n",
      "6000 of 19570\n",
      "6200 of 19570\n",
      "6400 of 19570\n",
      "6600 of 19570\n",
      "6800 of 19570\n",
      "7000 of 19570\n",
      "7200 of 19570\n",
      "7400 of 19570\n",
      "7600 of 19570\n"
     ]
    }
   ],
   "source": [
    "trips = pd.read_feather(\n",
    "    os.path.join(config.PROCESSED_DATA_PATH, 'trips.feather')\n",
    ")\n",
    "trips.set_index('rte_id', inplace=True)\n",
    "\n",
    "\n",
    "# If everything is working well, GPS points should be < 20 m apart\n",
    "# However, a user's tolerance is much higher than this - as long as\n",
    "# ride is easy to follow in map view, it is fine for this purpose.\n",
    "clusterer = sklearn.cluster.DBSCAN(eps=0.1) # 0.1 degree ~ 1 km\n",
    "\n",
    "bad_rtes = []\n",
    "for i, rte_id in enumerate(trips.index.tolist()):\n",
    "    if not i % 200: print('{} of {}'.format(i, trips.shape[0]))\n",
    "    \n",
    "    ride = pd.read_feather(\n",
    "        os.path.join(config.CLEAN_TRIPS_PATH, '{}.feather'.format(rte_id))\n",
    "    )\n",
    "        \n",
    "    clusterer.fit(ride[['lat', 'lon']])\n",
    "    ride['labels'] = clusterer.labels_\n",
    "    if ride.labels.nunique() == 1:\n",
    "        continue\n",
    "        \n",
    "    bad_rtes += [ride]\n",
    "    \n",
    "    # Find segment with largest number of breadcrumb points\n",
    "    biggest_segment = ride.labels.value_counts().index[0]\n",
    "    \n",
    "    ride = (ride[ride.labels == biggest_segment]\n",
    "            .reset_index(drop=True).drop('labels', axis=1))\n",
    "    \n",
    "    # If this now means that the ride is very short, drop it\n",
    "    if ride['dist'].sum() <= 1:\n",
    "        os.remove(os.path.join(config.CLEAN_DATA_PATH, '{}.feather'.format(rte_id)))\n",
    "        continue\n",
    "    \n",
    "    ride.to_feather(\n",
    "        os.path.join(config.CLEAN_DATA_PATH, '{}.feather'.format(rte_id))\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out some of those discontinuous trips visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colours = sns.color_palette('husl', 5)\n",
    "inds = np.random.choice(len(bad_rtes), 5, replace=False)\n",
    "\n",
    "for ind in inds:\n",
    "    ride = bad_rtes[ind]\n",
    "    for i, cluster_label in enumerate(ride.labels.value_counts().index.tolist()):\n",
    "        plt.plot(ride[ride.labels == cluster_label].lon,\n",
    "                 ride[ride.labels == cluster_label].lat,\n",
    "                 '.', markersize=0.5, \n",
    "                 color=np.array(colours[i]) * (a.labels.nunique() - i) / a.labels.nunique()\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And remove any of these deleted trips from the main DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips = pd.read_feather(\n",
    "    os.path.join(config.PROCESSED_DATA_PATH, 'trips.feather')\n",
    ")\n",
    "# List all [rte_id].feather files in CLEAN_TRIPS_PATH\n",
    "clean_trip_ids = [rte_id.stem for rte_id \n",
    "                  in pathlib.Path(config.CLEAN_TRIPS_PATH).glob('*.feather')]\n",
    "\n",
    "\n",
    "trips = trips[(trips['rte_id'].isin(clean_trip))].reset_index(drop=True)\n",
    "trips.to_feather(\n",
    "    os.path.join(config.PROCESSED_DATA_PATH, 'trips.feather')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the road backbone - CLIQUE\n",
    "\n",
    "The lat/lon breadcrumbs are inconsistent across all the rides.  To make life easier, need to build a backbone of consistent lat/lon points along all of the roads included in the database.\n",
    "\n",
    "CLIQUE clustering is ideal\n",
    "\n",
    "* Above some threshold (e.g. one ride), data density does not matter - so will not get more cluster centres where there are more rides\n",
    "* Cluster centres are evenly distributed in space at the granularity that you decide is appropriate\n",
    "\n",
    "For the basic road backbone, a granularity of around 200 m (0.002 degrees) seems appropriate - except very close to junctions, this can distinguish between different roads.\n",
    "\n",
    "Later, I will want to calculate distances between points with low latency - as the user puts their preferred start location into the app, I need to calculate how far it is to any given ride.  As distances of less than 1 mile are unlikely to be significant in this proximity question, we can use a coarser granularity (0.015 degrees).\n",
    "\n",
    "For the de-duping question, an even coarser grid can be used - I would rather throw out rides that are a little bit different than retain them.  This also makes the clustering at that stage easier.  I use a granularity of around 3.5 miles (0.05 degrees)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4687500000000004"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips = pd.read_feather(\n",
    "    os.path.join(config.PROCESSED_DATA_PATH, 'trips.feather')\n",
    ")\n",
    "trips.set_index('rte_id', inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to address de-duping first, as this will make everything else run much more quickly :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set, {})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "collections.defaultdict(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts_per_degree = 20\n",
    "\n",
    "grid_pts, gridpts_at_rte = road_backbone.make_road_backbone(\n",
    "    trips.index.tolist(), pts_per_degree\n",
    ")\n",
    "\n",
    "road_backbone.save_gridpts(grid_pts, grid_dict, grid_fname, rtes_at_grid_fname)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
